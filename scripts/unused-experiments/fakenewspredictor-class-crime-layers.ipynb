{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7817310,"sourceType":"datasetVersion","datasetId":4579803},{"sourceId":7826067,"sourceType":"datasetVersion","datasetId":4585948}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# loading dependencies\n!pip install accelerate==0.27.2 --progress-bar off\n!pip install -q peft==0.4.0 --progress-bar off\n# !pip install -q bitsandbytes==0.40.2 --progress-bar off\n!pip install -q bitsandbytes==0.41.3 --progress-bar off\n!pip install -q transformers==4.38.2 --progress-bar off\n!pip install -q trl==0.4.7 --progress-bar off","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:08:38.366908Z","iopub.execute_input":"2024-03-26T22:08:38.367172Z","iopub.status.idle":"2024-03-26T22:09:56.907857Z","shell.execute_reply.started":"2024-03-26T22:08:38.367148Z","shell.execute_reply":"2024-03-26T22:09:56.906832Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate==0.27.2 in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.27.2) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.27.2) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.27.2) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom random import randrange\nfrom functools import partial\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (AutoModelForCausalLM,\n                          AutoModelForSequenceClassification,\n                          AutoTokenizer,\n                          BitsAndBytesConfig,\n                          HfArgumentParser,\n                          Trainer,\n                          TrainingArguments,\n                          DataCollatorForLanguageModeling,\n                          DataCollatorWithPadding,\n                          EarlyStoppingCallback,\n                          pipeline,\n                          logging,\n                          set_seed)\n\nimport bitsandbytes as bnb\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification, TaskType\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:09:56.909927Z","iopub.execute_input":"2024-03-26T22:09:56.910722Z","iopub.status.idle":"2024-03-26T22:10:15.788538Z","shell.execute_reply.started":"2024-03-26T22:09:56.910681Z","shell.execute_reply":"2024-03-26T22:10:15.787556Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-26 22:10:06.286333: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-26 22:10:06.286448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-26 22:10:06.424716: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\nfrom imblearn.metrics import geometric_mean_score\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:10:19.005974Z","iopub.execute_input":"2024-03-26T22:10:19.007297Z","iopub.status.idle":"2024-03-26T22:10:19.310892Z","shell.execute_reply.started":"2024-03-26T22:10:19.007264Z","shell.execute_reply":"2024-03-26T22:10:19.310124Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:10:23.292268Z","iopub.execute_input":"2024-03-26T22:10:23.293000Z","iopub.status.idle":"2024-03-26T22:10:23.319495Z","shell.execute_reply.started":"2024-03-26T22:10:23.292968Z","shell.execute_reply":"2024-03-26T22:10:23.318623Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00c316ad70fa4da2b8104c7fed8bf92b"}},"metadata":{}}]},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-2-7b-hf\"\noutput_dir = \"/kaggle/working/results_class_e1_bce_7layers\"","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:16:07.981648Z","iopub.execute_input":"2024-03-26T22:16:07.982341Z","iopub.status.idle":"2024-03-26T22:16:07.986239Z","shell.execute_reply.started":"2024-03-26T22:16:07.982313Z","shell.execute_reply":"2024-03-26T22:16:07.985369Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"os.rename(output_dir,\"/kaggle/working/results_class_e1_bce_1layer\")","metadata":{"execution":{"iopub.status.busy":"2024-03-26T14:43:06.105647Z","iopub.execute_input":"2024-03-26T14:43:06.106029Z","iopub.status.idle":"2024-03-26T14:43:06.167576Z","shell.execute_reply.started":"2024-03-26T14:43:06.106001Z","shell.execute_reply":"2024-03-26T14:43:06.166334Z"},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/results_class_e1_bce_1layer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/results_class_e1_bce_all_layers' -> '/kaggle/working/results_class_e1_bce_1layer'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/results_class_e1_bce_all_layers' -> '/kaggle/working/results_class_e1_bce_1layer'","output_type":"error"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\n\n# Upload all the content from the local folder to your remote Space.\n# By default, files are uploaded at the root of the repo\napi.upload_folder(\n    folder_path=output_dir,\n    repo_id=\"FakeNewsLlama/CrimeTrueFake7Layers\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:38:43.684832Z","iopub.execute_input":"2024-03-26T22:38:43.685677Z","iopub.status.idle":"2024-03-26T22:38:48.678245Z","shell.execute_reply.started":"2024-03-26T22:38:43.685639Z","shell.execute_reply":"2024-03-26T22:38:48.677101Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1711491443.bd9d44df5192.34.0:   0%|          | 0.00/6.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc90321f5bd4aedaa39503d2a1aaeff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/35.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ac82573a4f947b7ab7b3f4eef8296fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd0b17c29ce4865b59e3a93b1911aa5"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/FakeNewsLlama/CrimeTrueFake7Layers/commit/3857e40cef0a2435ba1dd592cd68302008f211b2', commit_message='Upload folder using huggingface_hub', commit_description='', oid='3857e40cef0a2435ba1dd592cd68302008f211b2', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# only to remove directories\nimport shutil\n# shutil.rmtree('/kaggle/working/news_classification_llama2_7b')\n# shutil.rmtree('/kaggle/working/results_class_e1_bce')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n    \"\"\"\n    Configures model quantization method using bitsandbytes to speed up training and inference\n\n    :param load_in_4bit: Load model in 4-bit precision mode\n    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n    \"\"\"\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit = load_in_4bit,\n        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n        bnb_4bit_quant_type = bnb_4bit_quant_type,\n        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n    )\n\n    return bnb_config","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:10:58.980455Z","iopub.execute_input":"2024-03-26T22:10:58.980814Z","iopub.status.idle":"2024-03-26T22:10:58.987041Z","shell.execute_reply.started":"2024-03-26T22:10:58.980788Z","shell.execute_reply":"2024-03-26T22:10:58.985893Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Activate 4-bit precision base model loading\nload_in_4bit = True\n\n# Activate nested quantization for 4-bit base models (double quantization)\nbnb_4bit_use_double_quant = True\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Compute data type for 4-bit base models\nbnb_4bit_compute_dtype = torch.bfloat16\n\nbnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:11:02.435385Z","iopub.execute_input":"2024-03-26T22:11:02.436194Z","iopub.status.idle":"2024-03-26T22:11:02.442506Z","shell.execute_reply.started":"2024-03-26T22:11:02.436161Z","shell.execute_reply":"2024-03-26T22:11:02.441631Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"*loading model and tokenizer*","metadata":{}},{"cell_type":"code","source":"# loading model and tokenizer\nn_gpus = torch.cuda.device_count()\nmax_memory = f'{40960}MB'\n\nlabel2id = {'TRUE': 0, 'FAKE': 1}\nid2label = {0: 'TRUE', 1: 'FAKE'}\n\n#load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        quantization_config = bnb_config,\n        device_map = \"auto\", # dispatch the model efficiently on the available resources\n        max_memory = {i: max_memory for i in range(n_gpus)},\n        num_labels = 2,\n        label2id = label2id,\n        id2label = id2label\n    )\n\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel.config.problem_type = \"single_label_classification\"","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:11:05.015203Z","iopub.execute_input":"2024-03-26T22:11:05.015780Z","iopub.status.idle":"2024-03-26T22:13:09.087958Z","shell.execute_reply.started":"2024-03-26T22:11:05.015748Z","shell.execute_reply":"2024-03-26T22:13:09.087151Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d4fb17dc13476e869897717f6af33f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9713c2af4a054a7cab66bc8fcc94ba44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95128156dffc4986ae142a44ea474807"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5132d0687549dda71c93a0fa3885e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbc0c60fd2f4181b58469cde81efeec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"385826ea91ef47beac677973909c66f7"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"#load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:13:15.558145Z","iopub.execute_input":"2024-03-26T22:13:15.559066Z","iopub.status.idle":"2024-03-26T22:13:16.149952Z","shell.execute_reply.started":"2024-03-26T22:13:15.559027Z","shell.execute_reply":"2024-03-26T22:13:16.148926Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d856a9dd41048fd862923b891708fc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"315493a5e93c49bf99999d1b1645e16e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9b8d7ebc0e142b3b2609b3afb3d09c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd4298e1d3f41b59c8ed2b1a0ff67b3"}},"metadata":{}}]},{"cell_type":"code","source":"for name, module in model.named_modules():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T00:08:01.729227Z","iopub.execute_input":"2024-03-26T00:08:01.729972Z","iopub.status.idle":"2024-03-26T00:08:01.740728Z","shell.execute_reply.started":"2024-03-26T00:08:01.729936Z","shell.execute_reply":"2024-03-26T00:08:01.739775Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\nmodel\nmodel.embed_tokens\nmodel.layers\nmodel.layers.0\nmodel.layers.0.self_attn\nmodel.layers.0.self_attn.q_proj\nmodel.layers.0.self_attn.k_proj\nmodel.layers.0.self_attn.v_proj\nmodel.layers.0.self_attn.o_proj\nmodel.layers.0.self_attn.rotary_emb\nmodel.layers.0.mlp\nmodel.layers.0.mlp.gate_proj\nmodel.layers.0.mlp.up_proj\nmodel.layers.0.mlp.down_proj\nmodel.layers.0.mlp.act_fn\nmodel.layers.0.input_layernorm\nmodel.layers.0.post_attention_layernorm\nmodel.layers.1\nmodel.layers.1.self_attn\nmodel.layers.1.self_attn.q_proj\nmodel.layers.1.self_attn.k_proj\nmodel.layers.1.self_attn.v_proj\nmodel.layers.1.self_attn.o_proj\nmodel.layers.1.self_attn.rotary_emb\nmodel.layers.1.mlp\nmodel.layers.1.mlp.gate_proj\nmodel.layers.1.mlp.up_proj\nmodel.layers.1.mlp.down_proj\nmodel.layers.1.mlp.act_fn\nmodel.layers.1.input_layernorm\nmodel.layers.1.post_attention_layernorm\nmodel.layers.2\nmodel.layers.2.self_attn\nmodel.layers.2.self_attn.q_proj\nmodel.layers.2.self_attn.k_proj\nmodel.layers.2.self_attn.v_proj\nmodel.layers.2.self_attn.o_proj\nmodel.layers.2.self_attn.rotary_emb\nmodel.layers.2.mlp\nmodel.layers.2.mlp.gate_proj\nmodel.layers.2.mlp.up_proj\nmodel.layers.2.mlp.down_proj\nmodel.layers.2.mlp.act_fn\nmodel.layers.2.input_layernorm\nmodel.layers.2.post_attention_layernorm\nmodel.layers.3\nmodel.layers.3.self_attn\nmodel.layers.3.self_attn.q_proj\nmodel.layers.3.self_attn.k_proj\nmodel.layers.3.self_attn.v_proj\nmodel.layers.3.self_attn.o_proj\nmodel.layers.3.self_attn.rotary_emb\nmodel.layers.3.mlp\nmodel.layers.3.mlp.gate_proj\nmodel.layers.3.mlp.up_proj\nmodel.layers.3.mlp.down_proj\nmodel.layers.3.mlp.act_fn\nmodel.layers.3.input_layernorm\nmodel.layers.3.post_attention_layernorm\nmodel.layers.4\nmodel.layers.4.self_attn\nmodel.layers.4.self_attn.q_proj\nmodel.layers.4.self_attn.k_proj\nmodel.layers.4.self_attn.v_proj\nmodel.layers.4.self_attn.o_proj\nmodel.layers.4.self_attn.rotary_emb\nmodel.layers.4.mlp\nmodel.layers.4.mlp.gate_proj\nmodel.layers.4.mlp.up_proj\nmodel.layers.4.mlp.down_proj\nmodel.layers.4.mlp.act_fn\nmodel.layers.4.input_layernorm\nmodel.layers.4.post_attention_layernorm\nmodel.layers.5\nmodel.layers.5.self_attn\nmodel.layers.5.self_attn.q_proj\nmodel.layers.5.self_attn.k_proj\nmodel.layers.5.self_attn.v_proj\nmodel.layers.5.self_attn.o_proj\nmodel.layers.5.self_attn.rotary_emb\nmodel.layers.5.mlp\nmodel.layers.5.mlp.gate_proj\nmodel.layers.5.mlp.up_proj\nmodel.layers.5.mlp.down_proj\nmodel.layers.5.mlp.act_fn\nmodel.layers.5.input_layernorm\nmodel.layers.5.post_attention_layernorm\nmodel.layers.6\nmodel.layers.6.self_attn\nmodel.layers.6.self_attn.q_proj\nmodel.layers.6.self_attn.k_proj\nmodel.layers.6.self_attn.v_proj\nmodel.layers.6.self_attn.o_proj\nmodel.layers.6.self_attn.rotary_emb\nmodel.layers.6.mlp\nmodel.layers.6.mlp.gate_proj\nmodel.layers.6.mlp.up_proj\nmodel.layers.6.mlp.down_proj\nmodel.layers.6.mlp.act_fn\nmodel.layers.6.input_layernorm\nmodel.layers.6.post_attention_layernorm\nmodel.layers.7\nmodel.layers.7.self_attn\nmodel.layers.7.self_attn.q_proj\nmodel.layers.7.self_attn.k_proj\nmodel.layers.7.self_attn.v_proj\nmodel.layers.7.self_attn.o_proj\nmodel.layers.7.self_attn.rotary_emb\nmodel.layers.7.mlp\nmodel.layers.7.mlp.gate_proj\nmodel.layers.7.mlp.up_proj\nmodel.layers.7.mlp.down_proj\nmodel.layers.7.mlp.act_fn\nmodel.layers.7.input_layernorm\nmodel.layers.7.post_attention_layernorm\nmodel.layers.8\nmodel.layers.8.self_attn\nmodel.layers.8.self_attn.q_proj\nmodel.layers.8.self_attn.k_proj\nmodel.layers.8.self_attn.v_proj\nmodel.layers.8.self_attn.o_proj\nmodel.layers.8.self_attn.rotary_emb\nmodel.layers.8.mlp\nmodel.layers.8.mlp.gate_proj\nmodel.layers.8.mlp.up_proj\nmodel.layers.8.mlp.down_proj\nmodel.layers.8.mlp.act_fn\nmodel.layers.8.input_layernorm\nmodel.layers.8.post_attention_layernorm\nmodel.layers.9\nmodel.layers.9.self_attn\nmodel.layers.9.self_attn.q_proj\nmodel.layers.9.self_attn.k_proj\nmodel.layers.9.self_attn.v_proj\nmodel.layers.9.self_attn.o_proj\nmodel.layers.9.self_attn.rotary_emb\nmodel.layers.9.mlp\nmodel.layers.9.mlp.gate_proj\nmodel.layers.9.mlp.up_proj\nmodel.layers.9.mlp.down_proj\nmodel.layers.9.mlp.act_fn\nmodel.layers.9.input_layernorm\nmodel.layers.9.post_attention_layernorm\nmodel.layers.10\nmodel.layers.10.self_attn\nmodel.layers.10.self_attn.q_proj\nmodel.layers.10.self_attn.k_proj\nmodel.layers.10.self_attn.v_proj\nmodel.layers.10.self_attn.o_proj\nmodel.layers.10.self_attn.rotary_emb\nmodel.layers.10.mlp\nmodel.layers.10.mlp.gate_proj\nmodel.layers.10.mlp.up_proj\nmodel.layers.10.mlp.down_proj\nmodel.layers.10.mlp.act_fn\nmodel.layers.10.input_layernorm\nmodel.layers.10.post_attention_layernorm\nmodel.layers.11\nmodel.layers.11.self_attn\nmodel.layers.11.self_attn.q_proj\nmodel.layers.11.self_attn.k_proj\nmodel.layers.11.self_attn.v_proj\nmodel.layers.11.self_attn.o_proj\nmodel.layers.11.self_attn.rotary_emb\nmodel.layers.11.mlp\nmodel.layers.11.mlp.gate_proj\nmodel.layers.11.mlp.up_proj\nmodel.layers.11.mlp.down_proj\nmodel.layers.11.mlp.act_fn\nmodel.layers.11.input_layernorm\nmodel.layers.11.post_attention_layernorm\nmodel.layers.12\nmodel.layers.12.self_attn\nmodel.layers.12.self_attn.q_proj\nmodel.layers.12.self_attn.k_proj\nmodel.layers.12.self_attn.v_proj\nmodel.layers.12.self_attn.o_proj\nmodel.layers.12.self_attn.rotary_emb\nmodel.layers.12.mlp\nmodel.layers.12.mlp.gate_proj\nmodel.layers.12.mlp.up_proj\nmodel.layers.12.mlp.down_proj\nmodel.layers.12.mlp.act_fn\nmodel.layers.12.input_layernorm\nmodel.layers.12.post_attention_layernorm\nmodel.layers.13\nmodel.layers.13.self_attn\nmodel.layers.13.self_attn.q_proj\nmodel.layers.13.self_attn.k_proj\nmodel.layers.13.self_attn.v_proj\nmodel.layers.13.self_attn.o_proj\nmodel.layers.13.self_attn.rotary_emb\nmodel.layers.13.mlp\nmodel.layers.13.mlp.gate_proj\nmodel.layers.13.mlp.up_proj\nmodel.layers.13.mlp.down_proj\nmodel.layers.13.mlp.act_fn\nmodel.layers.13.input_layernorm\nmodel.layers.13.post_attention_layernorm\nmodel.layers.14\nmodel.layers.14.self_attn\nmodel.layers.14.self_attn.q_proj\nmodel.layers.14.self_attn.k_proj\nmodel.layers.14.self_attn.v_proj\nmodel.layers.14.self_attn.o_proj\nmodel.layers.14.self_attn.rotary_emb\nmodel.layers.14.mlp\nmodel.layers.14.mlp.gate_proj\nmodel.layers.14.mlp.up_proj\nmodel.layers.14.mlp.down_proj\nmodel.layers.14.mlp.act_fn\nmodel.layers.14.input_layernorm\nmodel.layers.14.post_attention_layernorm\nmodel.layers.15\nmodel.layers.15.self_attn\nmodel.layers.15.self_attn.q_proj\nmodel.layers.15.self_attn.k_proj\nmodel.layers.15.self_attn.v_proj\nmodel.layers.15.self_attn.o_proj\nmodel.layers.15.self_attn.rotary_emb\nmodel.layers.15.mlp\nmodel.layers.15.mlp.gate_proj\nmodel.layers.15.mlp.up_proj\nmodel.layers.15.mlp.down_proj\nmodel.layers.15.mlp.act_fn\nmodel.layers.15.input_layernorm\nmodel.layers.15.post_attention_layernorm\nmodel.layers.16\nmodel.layers.16.self_attn\nmodel.layers.16.self_attn.q_proj\nmodel.layers.16.self_attn.k_proj\nmodel.layers.16.self_attn.v_proj\nmodel.layers.16.self_attn.o_proj\nmodel.layers.16.self_attn.rotary_emb\nmodel.layers.16.mlp\nmodel.layers.16.mlp.gate_proj\nmodel.layers.16.mlp.up_proj\nmodel.layers.16.mlp.down_proj\nmodel.layers.16.mlp.act_fn\nmodel.layers.16.input_layernorm\nmodel.layers.16.post_attention_layernorm\nmodel.layers.17\nmodel.layers.17.self_attn\nmodel.layers.17.self_attn.q_proj\nmodel.layers.17.self_attn.k_proj\nmodel.layers.17.self_attn.v_proj\nmodel.layers.17.self_attn.o_proj\nmodel.layers.17.self_attn.rotary_emb\nmodel.layers.17.mlp\nmodel.layers.17.mlp.gate_proj\nmodel.layers.17.mlp.up_proj\nmodel.layers.17.mlp.down_proj\nmodel.layers.17.mlp.act_fn\nmodel.layers.17.input_layernorm\nmodel.layers.17.post_attention_layernorm\nmodel.layers.18\nmodel.layers.18.self_attn\nmodel.layers.18.self_attn.q_proj\nmodel.layers.18.self_attn.k_proj\nmodel.layers.18.self_attn.v_proj\nmodel.layers.18.self_attn.o_proj\nmodel.layers.18.self_attn.rotary_emb\nmodel.layers.18.mlp\nmodel.layers.18.mlp.gate_proj\nmodel.layers.18.mlp.up_proj\nmodel.layers.18.mlp.down_proj\nmodel.layers.18.mlp.act_fn\nmodel.layers.18.input_layernorm\nmodel.layers.18.post_attention_layernorm\nmodel.layers.19\nmodel.layers.19.self_attn\nmodel.layers.19.self_attn.q_proj\nmodel.layers.19.self_attn.k_proj\nmodel.layers.19.self_attn.v_proj\nmodel.layers.19.self_attn.o_proj\nmodel.layers.19.self_attn.rotary_emb\nmodel.layers.19.mlp\nmodel.layers.19.mlp.gate_proj\nmodel.layers.19.mlp.up_proj\nmodel.layers.19.mlp.down_proj\nmodel.layers.19.mlp.act_fn\nmodel.layers.19.input_layernorm\nmodel.layers.19.post_attention_layernorm\nmodel.layers.20\nmodel.layers.20.self_attn\nmodel.layers.20.self_attn.q_proj\nmodel.layers.20.self_attn.k_proj\nmodel.layers.20.self_attn.v_proj\nmodel.layers.20.self_attn.o_proj\nmodel.layers.20.self_attn.rotary_emb\nmodel.layers.20.mlp\nmodel.layers.20.mlp.gate_proj\nmodel.layers.20.mlp.up_proj\nmodel.layers.20.mlp.down_proj\nmodel.layers.20.mlp.act_fn\nmodel.layers.20.input_layernorm\nmodel.layers.20.post_attention_layernorm\nmodel.layers.21\nmodel.layers.21.self_attn\nmodel.layers.21.self_attn.q_proj\nmodel.layers.21.self_attn.k_proj\nmodel.layers.21.self_attn.v_proj\nmodel.layers.21.self_attn.o_proj\nmodel.layers.21.self_attn.rotary_emb\nmodel.layers.21.mlp\nmodel.layers.21.mlp.gate_proj\nmodel.layers.21.mlp.up_proj\nmodel.layers.21.mlp.down_proj\nmodel.layers.21.mlp.act_fn\nmodel.layers.21.input_layernorm\nmodel.layers.21.post_attention_layernorm\nmodel.layers.22\nmodel.layers.22.self_attn\nmodel.layers.22.self_attn.q_proj\nmodel.layers.22.self_attn.k_proj\nmodel.layers.22.self_attn.v_proj\nmodel.layers.22.self_attn.o_proj\nmodel.layers.22.self_attn.rotary_emb\nmodel.layers.22.mlp\nmodel.layers.22.mlp.gate_proj\nmodel.layers.22.mlp.up_proj\nmodel.layers.22.mlp.down_proj\nmodel.layers.22.mlp.act_fn\nmodel.layers.22.input_layernorm\nmodel.layers.22.post_attention_layernorm\nmodel.layers.23\nmodel.layers.23.self_attn\nmodel.layers.23.self_attn.q_proj\nmodel.layers.23.self_attn.k_proj\nmodel.layers.23.self_attn.v_proj\nmodel.layers.23.self_attn.o_proj\nmodel.layers.23.self_attn.rotary_emb\nmodel.layers.23.mlp\nmodel.layers.23.mlp.gate_proj\nmodel.layers.23.mlp.up_proj\nmodel.layers.23.mlp.down_proj\nmodel.layers.23.mlp.act_fn\nmodel.layers.23.input_layernorm\nmodel.layers.23.post_attention_layernorm\nmodel.layers.24\nmodel.layers.24.self_attn\nmodel.layers.24.self_attn.q_proj\nmodel.layers.24.self_attn.k_proj\nmodel.layers.24.self_attn.v_proj\nmodel.layers.24.self_attn.o_proj\nmodel.layers.24.self_attn.rotary_emb\nmodel.layers.24.mlp\nmodel.layers.24.mlp.gate_proj\nmodel.layers.24.mlp.up_proj\nmodel.layers.24.mlp.down_proj\nmodel.layers.24.mlp.act_fn\nmodel.layers.24.input_layernorm\nmodel.layers.24.post_attention_layernorm\nmodel.layers.25\nmodel.layers.25.self_attn\nmodel.layers.25.self_attn.q_proj\nmodel.layers.25.self_attn.k_proj\nmodel.layers.25.self_attn.v_proj\nmodel.layers.25.self_attn.o_proj\nmodel.layers.25.self_attn.rotary_emb\nmodel.layers.25.mlp\nmodel.layers.25.mlp.gate_proj\nmodel.layers.25.mlp.up_proj\nmodel.layers.25.mlp.down_proj\nmodel.layers.25.mlp.act_fn\nmodel.layers.25.input_layernorm\nmodel.layers.25.post_attention_layernorm\nmodel.layers.26\nmodel.layers.26.self_attn\nmodel.layers.26.self_attn.q_proj\nmodel.layers.26.self_attn.k_proj\nmodel.layers.26.self_attn.v_proj\nmodel.layers.26.self_attn.o_proj\nmodel.layers.26.self_attn.rotary_emb\nmodel.layers.26.mlp\nmodel.layers.26.mlp.gate_proj\nmodel.layers.26.mlp.up_proj\nmodel.layers.26.mlp.down_proj\nmodel.layers.26.mlp.act_fn\nmodel.layers.26.input_layernorm\nmodel.layers.26.post_attention_layernorm\nmodel.layers.27\nmodel.layers.27.self_attn\nmodel.layers.27.self_attn.q_proj\nmodel.layers.27.self_attn.k_proj\nmodel.layers.27.self_attn.v_proj\nmodel.layers.27.self_attn.o_proj\nmodel.layers.27.self_attn.rotary_emb\nmodel.layers.27.mlp\nmodel.layers.27.mlp.gate_proj\nmodel.layers.27.mlp.up_proj\nmodel.layers.27.mlp.down_proj\nmodel.layers.27.mlp.act_fn\nmodel.layers.27.input_layernorm\nmodel.layers.27.post_attention_layernorm\nmodel.layers.28\nmodel.layers.28.self_attn\nmodel.layers.28.self_attn.q_proj\nmodel.layers.28.self_attn.k_proj\nmodel.layers.28.self_attn.v_proj\nmodel.layers.28.self_attn.o_proj\nmodel.layers.28.self_attn.rotary_emb\nmodel.layers.28.mlp\nmodel.layers.28.mlp.gate_proj\nmodel.layers.28.mlp.up_proj\nmodel.layers.28.mlp.down_proj\nmodel.layers.28.mlp.act_fn\nmodel.layers.28.input_layernorm\nmodel.layers.28.post_attention_layernorm\nmodel.layers.29\nmodel.layers.29.self_attn\nmodel.layers.29.self_attn.q_proj\nmodel.layers.29.self_attn.k_proj\nmodel.layers.29.self_attn.v_proj\nmodel.layers.29.self_attn.o_proj\nmodel.layers.29.self_attn.rotary_emb\nmodel.layers.29.mlp\nmodel.layers.29.mlp.gate_proj\nmodel.layers.29.mlp.up_proj\nmodel.layers.29.mlp.down_proj\nmodel.layers.29.mlp.act_fn\nmodel.layers.29.input_layernorm\nmodel.layers.29.post_attention_layernorm\nmodel.layers.30\nmodel.layers.30.self_attn\nmodel.layers.30.self_attn.q_proj\nmodel.layers.30.self_attn.k_proj\nmodel.layers.30.self_attn.v_proj\nmodel.layers.30.self_attn.o_proj\nmodel.layers.30.self_attn.rotary_emb\nmodel.layers.30.mlp\nmodel.layers.30.mlp.gate_proj\nmodel.layers.30.mlp.up_proj\nmodel.layers.30.mlp.down_proj\nmodel.layers.30.mlp.act_fn\nmodel.layers.30.input_layernorm\nmodel.layers.30.post_attention_layernorm\nmodel.layers.31\nmodel.layers.31.self_attn\nmodel.layers.31.self_attn.q_proj\nmodel.layers.31.self_attn.k_proj\nmodel.layers.31.self_attn.v_proj\nmodel.layers.31.self_attn.o_proj\nmodel.layers.31.self_attn.rotary_emb\nmodel.layers.31.mlp\nmodel.layers.31.mlp.gate_proj\nmodel.layers.31.mlp.up_proj\nmodel.layers.31.mlp.down_proj\nmodel.layers.31.mlp.act_fn\nmodel.layers.31.input_layernorm\nmodel.layers.31.post_attention_layernorm\nmodel.norm\nscore\n","output_type":"stream"}]},{"cell_type":"code","source":"find_all_linear_names(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T00:16:27.358064Z","iopub.execute_input":"2024-03-26T00:16:27.358802Z","iopub.status.idle":"2024-03-26T00:16:27.368225Z","shell.execute_reply.started":"2024-03-26T00:16:27.358772Z","shell.execute_reply":"2024-03-26T00:16:27.367280Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"model.layers.0.self_attn.q_proj\nmodel.layers.0.self_attn.k_proj\nmodel.layers.0.self_attn.v_proj\nmodel.layers.0.self_attn.o_proj\nmodel.layers.0.mlp.gate_proj\nmodel.layers.0.mlp.up_proj\nmodel.layers.0.mlp.down_proj\nmodel.layers.1.self_attn.q_proj\nmodel.layers.1.self_attn.k_proj\nmodel.layers.1.self_attn.v_proj\nmodel.layers.1.self_attn.o_proj\nmodel.layers.1.mlp.gate_proj\nmodel.layers.1.mlp.up_proj\nmodel.layers.1.mlp.down_proj\nmodel.layers.2.self_attn.q_proj\nmodel.layers.2.self_attn.k_proj\nmodel.layers.2.self_attn.v_proj\nmodel.layers.2.self_attn.o_proj\nmodel.layers.2.mlp.gate_proj\nmodel.layers.2.mlp.up_proj\nmodel.layers.2.mlp.down_proj\nmodel.layers.3.self_attn.q_proj\nmodel.layers.3.self_attn.k_proj\nmodel.layers.3.self_attn.v_proj\nmodel.layers.3.self_attn.o_proj\nmodel.layers.3.mlp.gate_proj\nmodel.layers.3.mlp.up_proj\nmodel.layers.3.mlp.down_proj\nmodel.layers.4.self_attn.q_proj\nmodel.layers.4.self_attn.k_proj\nmodel.layers.4.self_attn.v_proj\nmodel.layers.4.self_attn.o_proj\nmodel.layers.4.mlp.gate_proj\nmodel.layers.4.mlp.up_proj\nmodel.layers.4.mlp.down_proj\nmodel.layers.5.self_attn.q_proj\nmodel.layers.5.self_attn.k_proj\nmodel.layers.5.self_attn.v_proj\nmodel.layers.5.self_attn.o_proj\nmodel.layers.5.mlp.gate_proj\nmodel.layers.5.mlp.up_proj\nmodel.layers.5.mlp.down_proj\nmodel.layers.6.self_attn.q_proj\nmodel.layers.6.self_attn.k_proj\nmodel.layers.6.self_attn.v_proj\nmodel.layers.6.self_attn.o_proj\nmodel.layers.6.mlp.gate_proj\nmodel.layers.6.mlp.up_proj\nmodel.layers.6.mlp.down_proj\nmodel.layers.7.self_attn.q_proj\nmodel.layers.7.self_attn.k_proj\nmodel.layers.7.self_attn.v_proj\nmodel.layers.7.self_attn.o_proj\nmodel.layers.7.mlp.gate_proj\nmodel.layers.7.mlp.up_proj\nmodel.layers.7.mlp.down_proj\nmodel.layers.8.self_attn.q_proj\nmodel.layers.8.self_attn.k_proj\nmodel.layers.8.self_attn.v_proj\nmodel.layers.8.self_attn.o_proj\nmodel.layers.8.mlp.gate_proj\nmodel.layers.8.mlp.up_proj\nmodel.layers.8.mlp.down_proj\nmodel.layers.9.self_attn.q_proj\nmodel.layers.9.self_attn.k_proj\nmodel.layers.9.self_attn.v_proj\nmodel.layers.9.self_attn.o_proj\nmodel.layers.9.mlp.gate_proj\nmodel.layers.9.mlp.up_proj\nmodel.layers.9.mlp.down_proj\nmodel.layers.10.self_attn.q_proj\nmodel.layers.10.self_attn.k_proj\nmodel.layers.10.self_attn.v_proj\nmodel.layers.10.self_attn.o_proj\nmodel.layers.10.mlp.gate_proj\nmodel.layers.10.mlp.up_proj\nmodel.layers.10.mlp.down_proj\nmodel.layers.11.self_attn.q_proj\nmodel.layers.11.self_attn.k_proj\nmodel.layers.11.self_attn.v_proj\nmodel.layers.11.self_attn.o_proj\nmodel.layers.11.mlp.gate_proj\nmodel.layers.11.mlp.up_proj\nmodel.layers.11.mlp.down_proj\nmodel.layers.12.self_attn.q_proj\nmodel.layers.12.self_attn.k_proj\nmodel.layers.12.self_attn.v_proj\nmodel.layers.12.self_attn.o_proj\nmodel.layers.12.mlp.gate_proj\nmodel.layers.12.mlp.up_proj\nmodel.layers.12.mlp.down_proj\nmodel.layers.13.self_attn.q_proj\nmodel.layers.13.self_attn.k_proj\nmodel.layers.13.self_attn.v_proj\nmodel.layers.13.self_attn.o_proj\nmodel.layers.13.mlp.gate_proj\nmodel.layers.13.mlp.up_proj\nmodel.layers.13.mlp.down_proj\nmodel.layers.14.self_attn.q_proj\nmodel.layers.14.self_attn.k_proj\nmodel.layers.14.self_attn.v_proj\nmodel.layers.14.self_attn.o_proj\nmodel.layers.14.mlp.gate_proj\nmodel.layers.14.mlp.up_proj\nmodel.layers.14.mlp.down_proj\nmodel.layers.15.self_attn.q_proj\nmodel.layers.15.self_attn.k_proj\nmodel.layers.15.self_attn.v_proj\nmodel.layers.15.self_attn.o_proj\nmodel.layers.15.mlp.gate_proj\nmodel.layers.15.mlp.up_proj\nmodel.layers.15.mlp.down_proj\nmodel.layers.16.self_attn.q_proj\nmodel.layers.16.self_attn.k_proj\nmodel.layers.16.self_attn.v_proj\nmodel.layers.16.self_attn.o_proj\nmodel.layers.16.mlp.gate_proj\nmodel.layers.16.mlp.up_proj\nmodel.layers.16.mlp.down_proj\nmodel.layers.17.self_attn.q_proj\nmodel.layers.17.self_attn.k_proj\nmodel.layers.17.self_attn.v_proj\nmodel.layers.17.self_attn.o_proj\nmodel.layers.17.mlp.gate_proj\nmodel.layers.17.mlp.up_proj\nmodel.layers.17.mlp.down_proj\nmodel.layers.18.self_attn.q_proj\nmodel.layers.18.self_attn.k_proj\nmodel.layers.18.self_attn.v_proj\nmodel.layers.18.self_attn.o_proj\nmodel.layers.18.mlp.gate_proj\nmodel.layers.18.mlp.up_proj\nmodel.layers.18.mlp.down_proj\nmodel.layers.19.self_attn.q_proj\nmodel.layers.19.self_attn.k_proj\nmodel.layers.19.self_attn.v_proj\nmodel.layers.19.self_attn.o_proj\nmodel.layers.19.mlp.gate_proj\nmodel.layers.19.mlp.up_proj\nmodel.layers.19.mlp.down_proj\nmodel.layers.20.self_attn.q_proj\nmodel.layers.20.self_attn.k_proj\nmodel.layers.20.self_attn.v_proj\nmodel.layers.20.self_attn.o_proj\nmodel.layers.20.mlp.gate_proj\nmodel.layers.20.mlp.up_proj\nmodel.layers.20.mlp.down_proj\nmodel.layers.21.self_attn.q_proj\nmodel.layers.21.self_attn.k_proj\nmodel.layers.21.self_attn.v_proj\nmodel.layers.21.self_attn.o_proj\nmodel.layers.21.mlp.gate_proj\nmodel.layers.21.mlp.up_proj\nmodel.layers.21.mlp.down_proj\nmodel.layers.22.self_attn.q_proj\nmodel.layers.22.self_attn.k_proj\nmodel.layers.22.self_attn.v_proj\nmodel.layers.22.self_attn.o_proj\nmodel.layers.22.mlp.gate_proj\nmodel.layers.22.mlp.up_proj\nmodel.layers.22.mlp.down_proj\nmodel.layers.23.self_attn.q_proj\nmodel.layers.23.self_attn.k_proj\nmodel.layers.23.self_attn.v_proj\nmodel.layers.23.self_attn.o_proj\nmodel.layers.23.mlp.gate_proj\nmodel.layers.23.mlp.up_proj\nmodel.layers.23.mlp.down_proj\nmodel.layers.24.self_attn.q_proj\nmodel.layers.24.self_attn.k_proj\nmodel.layers.24.self_attn.v_proj\nmodel.layers.24.self_attn.o_proj\nmodel.layers.24.mlp.gate_proj\nmodel.layers.24.mlp.up_proj\nmodel.layers.24.mlp.down_proj\nmodel.layers.25.self_attn.q_proj\nmodel.layers.25.self_attn.k_proj\nmodel.layers.25.self_attn.v_proj\nmodel.layers.25.self_attn.o_proj\nmodel.layers.25.mlp.gate_proj\nmodel.layers.25.mlp.up_proj\nmodel.layers.25.mlp.down_proj\nmodel.layers.26.self_attn.q_proj\nmodel.layers.26.self_attn.k_proj\nmodel.layers.26.self_attn.v_proj\nmodel.layers.26.self_attn.o_proj\nmodel.layers.26.mlp.gate_proj\nmodel.layers.26.mlp.up_proj\nmodel.layers.26.mlp.down_proj\nmodel.layers.27.self_attn.q_proj\nmodel.layers.27.self_attn.k_proj\nmodel.layers.27.self_attn.v_proj\nmodel.layers.27.self_attn.o_proj\nmodel.layers.27.mlp.gate_proj\nmodel.layers.27.mlp.up_proj\nmodel.layers.27.mlp.down_proj\nmodel.layers.28.self_attn.q_proj\nmodel.layers.28.self_attn.k_proj\nmodel.layers.28.self_attn.v_proj\nmodel.layers.28.self_attn.o_proj\nmodel.layers.28.mlp.gate_proj\nmodel.layers.28.mlp.up_proj\nmodel.layers.28.mlp.down_proj\nmodel.layers.29.self_attn.q_proj\nmodel.layers.29.self_attn.k_proj\nmodel.layers.29.self_attn.v_proj\nmodel.layers.29.self_attn.o_proj\nmodel.layers.29.mlp.gate_proj\nmodel.layers.29.mlp.up_proj\nmodel.layers.29.mlp.down_proj\nmodel.layers.30.self_attn.q_proj\nmodel.layers.30.self_attn.k_proj\nmodel.layers.30.self_attn.v_proj\nmodel.layers.30.self_attn.o_proj\nmodel.layers.30.mlp.gate_proj\nmodel.layers.30.mlp.up_proj\nmodel.layers.30.mlp.down_proj\nmodel.layers.31.self_attn.q_proj\nmodel.layers.31.self_attn.k_proj\nmodel.layers.31.self_attn.v_proj\nmodel.layers.31.self_attn.o_proj\nmodel.layers.31.mlp.gate_proj\nmodel.layers.31.mlp.up_proj\nmodel.layers.31.mlp.down_proj\nLoRA module names: ['down_proj', 'v_proj', 'gate_proj', 'up_proj', 'k_proj', 'o_proj', 'q_proj']\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['down_proj', 'v_proj', 'gate_proj', 'up_proj', 'k_proj', 'o_proj', 'q_proj']"},"metadata":{}}]},{"cell_type":"code","source":"from torchinfo import summary\nsummary(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading train data\n\ntrain_df = pd.read_csv(\"/kaggle/input/crime-split/train_crime_df.csv\")\ndisplay(train_df['label'].value_counts())\n\ntarget_map = { 'TRUE': 0, 'FAKE': 1 }\ntarget_map_hot = { 'TRUE': [1,0], 'FAKE': [0,1] }\ntrain_df['target'] = train_df['label'].map(target_map)\n\ntrain_df = train_df.drop(['label'], axis=1)\ntrain_df = train_df.rename(columns={'target': 'labels'})\n\ndisplay(train_df.dtypes)\ndisplay(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:13:29.354843Z","iopub.execute_input":"2024-03-26T22:13:29.355697Z","iopub.status.idle":"2024-03-26T22:13:29.446679Z","shell.execute_reply.started":"2024-03-26T22:13:29.355658Z","shell.execute_reply":"2024-03-26T22:13:29.445633Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"label\nTRUE    341\nFAKE    302\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unnamed: 0     int64\ntext          object\nmetadata      object\ndomain        object\nlabels         int64\ndtype: object"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   Unnamed: 0                                               text  \\\n0         283  st by security HQ in central Syria NGO AFP Thu...   \n1         163  1 civilians in east Syria Monitor AFP Friday 1...   \n2         681  ort URL 0 15 The death toll following Daesh Th...   \n3         555  slamic State beheads crucifies in push for Syr...   \n4         259   men in Syrias Aleppo NGO AFP Sunday 29 Jun 20...   \n\n                                            metadata domain  labels  \n0  [{'article': None, 'author': None, 'date': '3/...  CRIME       1  \n1  [{'article': None, 'author': None, 'date': '7/...  CRIME       1  \n2  [{'article': None, 'author': None, 'date': '5/...  CRIME       1  \n3  [{'article': None, 'author': None, 'date': '8/...  CRIME       0  \n4  [{'article': None, 'author': None, 'date': '6/...  CRIME       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>metadata</th>\n      <th>domain</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>283</td>\n      <td>st by security HQ in central Syria NGO AFP Thu...</td>\n      <td>[{'article': None, 'author': None, 'date': '3/...</td>\n      <td>CRIME</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>163</td>\n      <td>1 civilians in east Syria Monitor AFP Friday 1...</td>\n      <td>[{'article': None, 'author': None, 'date': '7/...</td>\n      <td>CRIME</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>681</td>\n      <td>ort URL 0 15 The death toll following Daesh Th...</td>\n      <td>[{'article': None, 'author': None, 'date': '5/...</td>\n      <td>CRIME</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>555</td>\n      <td>slamic State beheads crucifies in push for Syr...</td>\n      <td>[{'article': None, 'author': None, 'date': '8/...</td>\n      <td>CRIME</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>259</td>\n      <td>men in Syrias Aleppo NGO AFP Sunday 29 Jun 20...</td>\n      <td>[{'article': None, 'author': None, 'date': '6/...</td>\n      <td>CRIME</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Loading valid data\n\nvalid_df = pd.read_csv(\"/kaggle/input/crime-split/valid_crime_df.csv\")\ndisplay(valid_df['label'].value_counts())\n\nvalid_df['target'] = valid_df['label'].map(target_map)\n\nvalid_df = valid_df.drop(['label'], axis=1)\nvalid_df = valid_df.rename(columns={'target': 'labels'})\n\ndisplay(valid_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:13:32.821810Z","iopub.execute_input":"2024-03-26T22:13:32.822648Z","iopub.status.idle":"2024-03-26T22:13:32.847298Z","shell.execute_reply.started":"2024-03-26T22:13:32.822615Z","shell.execute_reply":"2024-03-26T22:13:32.846279Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"label\nTRUE    42\nFAKE    38\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   Unnamed: 0                                               text  \\\n0         528  Russian Defense Ministry: More than 100 ISIS t...   \n1          84  ion 7 April 2017 A salvo of US cruise missiles...   \n2         615  Over 100 ISIL Terrorists Killed in Russian Off...   \n3         748  Used Chemical Weapons against Syria Kurds in H...   \n4         348  5 2017 The jet fighters from the Syrian regime...   \n\n                                            metadata domain  labels  \n0  [{'article': None, 'author': None, 'date': '10...  CRIME       0  \n1  [{'article': None, 'author': None, 'date': '4/...  CRIME       0  \n2  [{'article': None, 'author': None, 'date': '3/...  CRIME       0  \n3  [{'article': None, 'author': None, 'date': '7/...  CRIME       1  \n4  [{'article': None, 'author': None, 'date': '4/...  CRIME       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>metadata</th>\n      <th>domain</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>528</td>\n      <td>Russian Defense Ministry: More than 100 ISIS t...</td>\n      <td>[{'article': None, 'author': None, 'date': '10...</td>\n      <td>CRIME</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>84</td>\n      <td>ion 7 April 2017 A salvo of US cruise missiles...</td>\n      <td>[{'article': None, 'author': None, 'date': '4/...</td>\n      <td>CRIME</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>615</td>\n      <td>Over 100 ISIL Terrorists Killed in Russian Off...</td>\n      <td>[{'article': None, 'author': None, 'date': '3/...</td>\n      <td>CRIME</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>748</td>\n      <td>Used Chemical Weapons against Syria Kurds in H...</td>\n      <td>[{'article': None, 'author': None, 'date': '7/...</td>\n      <td>CRIME</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>348</td>\n      <td>5 2017 The jet fighters from the Syrian regime...</td>\n      <td>[{'article': None, 'author': None, 'date': '4/...</td>\n      <td>CRIME</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# convert to dataset\nimport datasets\nfrom datasets import Dataset, DatasetDict\n\ntrain_ds = Dataset.from_pandas(train_df)\nvalid_ds = Dataset.from_pandas(valid_df)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:13:36.933289Z","iopub.execute_input":"2024-03-26T22:13:36.933658Z","iopub.status.idle":"2024-03-26T22:13:36.947959Z","shell.execute_reply.started":"2024-03-26T22:13:36.933631Z","shell.execute_reply":"2024-03-26T22:13:36.947183Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_max_length(model):\n    \"\"\"\n    Extracts maximum token length from the model configuration\n\n    :param model: Hugging Face model\n    \"\"\"\n\n    # Pull model configuration\n    conf = model.config\n    # Initialize a \"max_length\" variable to store maximum sequence length as null\n    max_length = None\n    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:13:41.119702Z","iopub.execute_input":"2024-03-26T22:13:41.120075Z","iopub.status.idle":"2024-03-26T22:13:41.129314Z","shell.execute_reply.started":"2024-03-26T22:13:41.120048Z","shell.execute_reply":"2024-03-26T22:13:41.128354Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizes dataset batch\n\n    :param batch: Dataset batch\n    :param tokenizer: Model tokenizer\n    :param max_length: Maximum number of tokens to emit from the tokenizer\n    \"\"\"\n    \n    encoding = tokenizer(\n        batch[\"text\"],\n        max_length = max_length,\n        truncation = True,\n    )\n\n    return encoding","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:13:45.985991Z","iopub.execute_input":"2024-03-26T22:13:45.986485Z","iopub.status.idle":"2024-03-26T22:13:45.992502Z","shell.execute_reply.started":"2024-03-26T22:13:45.986435Z","shell.execute_reply":"2024-03-26T22:13:45.991438Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n    \"\"\"\n    Tokenizes dataset for fine-tuning\n\n    :param tokenizer (AutoTokenizer): Model tokenizer\n    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n    :param seed: Random seed for reproducibility\n    :param dataset (str): Instruction dataset\n    \"\"\"\n\n    col_to_delete = ['Unnamed: 0', 'text', 'metadata', 'domain']\n    \n    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n    # Apply the preprocessing function and remove the undesired columns\n    dataset_tokenized = dataset.map(_preprocessing_function, batched=True, remove_columns=col_to_delete)\n    # Rename the target to label as for HugginFace standards\n    dataset_tokenized = dataset_tokenized.rename_column(\"labels\", \"label\")\n    # Set to torch format\n    dataset_tokenized.set_format(\"torch\")\n    \n    \n#     # Add prompt to each sample\n#     print(\"Preprocessing dataset...\")\n    \n#     # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n#     _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n#     dataset_tokenized = dataset.map( # call _preprocessing_function on each sample in the dataset\n#         _preprocessing_function,\n#         batched = True,\n#         remove_columns = ['Unnamed: 0', 'text', 'metadata', 'domain']\n#     )    \n\n#     # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n#     dataset_tokenized = dataset_tokenized.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n    # Shuffle dataset\n    dataset_tokenized = dataset_tokenized.shuffle(seed = seed)\n    \n#     dataset_tokenized[\"labels\"] = torch.LongTensor(dataset_tokenized[\"labels\"]) \n\n    return dataset_tokenized","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:13:50.037429Z","iopub.execute_input":"2024-03-26T22:13:50.038269Z","iopub.status.idle":"2024-03-26T22:13:50.045023Z","shell.execute_reply.started":"2024-03-26T22:13:50.038235Z","shell.execute_reply":"2024-03-26T22:13:50.044071Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# preprocessing\n\n# Random seed\nseed = 33\n\n# max_length = get_max_length(model)\nmax_length = 512\npreprocessed_train_dataset = preprocess_dataset(tokenizer, max_length, seed, train_ds)\npreprocessed_validate_dataset = preprocess_dataset(tokenizer, max_length, seed, valid_ds)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:13:54.450612Z","iopub.execute_input":"2024-03-26T22:13:54.451239Z","iopub.status.idle":"2024-03-26T22:13:54.979774Z","shell.execute_reply.started":"2024-03-26T22:13:54.451206Z","shell.execute_reply":"2024-03-26T22:13:54.978882Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a4b75b8b16f497bbe15ea91ac528f30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7943691392d4c6686a11ca8150f4c97"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\nnp.object = object\ndisplay(preprocessed_train_dataset[0])\ndisplay(preprocessed_validate_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:14:03.637462Z","iopub.execute_input":"2024-03-26T22:14:03.637829Z","iopub.status.idle":"2024-03-26T22:14:03.661569Z","shell.execute_reply.started":"2024-03-26T22:14:03.637802Z","shell.execute_reply":"2024-03-26T22:14:03.660541Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"{'label': tensor(1),\n 'input_ids': tensor([    1,   263,   952,   310,  1067,  1161,   267,   297,  8713, 15851,\n           360,  2518, 29874,  2598,  2105, 23844, 29925, 24211, 29871, 29906,\n         29953,   319,   558, 29871, 29906, 29900, 29896, 29946,  2180,  3203,\n         29871, 29947, 29947,   337,  6596,   322, 22384,  8249,   505,  1063,\n          9445,   297,  1023,  3841,   310,  1067,  1161,   267,   363,  2761,\n           310, 16650,   293, 11840,   297,  8713, 15851, 14841,   360,  2518,\n         29874, 12291,   385, 29652,  2318,  1497, 24211, 29889,   450,  8713,\n          6392, 21651,  7606,   363, 12968, 26863,  2729,   297, 14933,  1497,\n         29871, 29946, 29945, 19626,   285,  1141,  2153,   322, 29871, 29946,\n         29941, 22384,  8249,   750,  1063,  9445,   297,   278, 17770,   393,\n          4689,   373,   498,  1295,  3250, 29889, 21651,  7606,  8881,   390,\n          4479,  1976,  6144, 16790,  1171,  5429, 23844, 29925,   393,   337,\n          6596,  8249,  3704,   285,  1141,  2153,   515,   838, 29899, 29984,\n         29874,   287,   294,  8713,  2849, 23736,   403,   838, 29899, 29940,\n           375,   336, 13960,   750, 25291,   278, 16650,   293, 10288,   394,\n         29899, 29967, 19266,  4099, 29882, 17306,   373,   498,  1295,  3250,\n         29889,  2233,  1161,   267,   892,  3133,   292,   408,   278, 19626,\n           285,  1141,  2153, 18365,   304,  2125,  1790, 17306,  3332, 20810,\n           297,   263, 21000,   304,  4511, 20123,   896,  4808,   297,   360,\n          2518, 29874,   322,   278,   660,  1540,   277,   336,  5120, 19963,\n           278, 22895,  5037, 29899, 16770,  1000, 20268,   273,   940,  5861,\n         29889,   450, 21651,  7606,  1497,   337,  6596,  8249,   750, 25291,\n         25340,   322,   626, 24579,   654,  2645,   278,  1067,  1161,   267,\n           322,   393, 22384,  8249,   750,  2000,   297,  2304,   515,  9416,\n          1616, 19486,   322,  1081,   293,   459,  2153,   304,  1018,   304,\n           337, 19730,   278, 17306, 29889,  1551, 24211,  1976,  6144, 16790,\n          1171,  1497,   278, 19626,   471, 12789,  4746,   967, 14231,   373,\n          4332,  3864, 10288, 16131, 29884,   777,  5320, 24016,   313, 17536,\n          7800, 29897,   515, 10288,   394, 29899, 29967, 19266,  4099, 29882,\n           297,   263, 21000,   304,  1544, 10161,  1090,   967,  2761, 29889]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"{'label': tensor(1),\n 'input_ids': tensor([    1,   330,   504, 29871, 29906, 29896, 29871, 29906, 29900, 29896,\n         29941,   278,  4007,   328, 22384, 17285,  7679,  1691, 10423,   411,\n          7745,   262, 10489,   373,   337,  6596, 29899, 29882,  2495, 13491,\n           449, 29874, 17692,  9865,  6151,   375, 23393,   472,  3203, 29871,\n         29896, 29946, 29900, 29900, 14175,  2638,   550,  3704,  5866,   322,\n          4344, 29889, 29871, 29906, 29889,  4367,  3454, 29901, 10504,   408,\n          1532,   408, 10387,  5922, 24905,  9571,   278, 16661,   322,   408,\n           372,   471, 24067,   263,  2654,  1196, 12061,  8859,   491,  3148,\n          7178,  2261,   547,  4250,  3304,   278,  3303,  3900, 13240,   363,\n           263,  6035,  3321, 21283,  8401,   967,  1370,  9981,   297,   278,\n         25620, 10800,   273, 17649,   304,  8713,  2849, 29889, 29871, 29941,\n         29889,   624,  5357,   267, 29901,  1551,  5533, 29871, 29929, 29871,\n         29906, 29900, 29896, 29941,  3148, 19358, 17719,  2259, 12693,   719,\n          1754,   263, 13182,   472,   278, 13331,   310,  4759,  1259,   278,\n         21283,   565,   278,  4007,   328, 22384,   892,   304,  1361,   975,\n           967, 22233,   564,  4881,   284, 29889, 11095,   297,  3839, 29871,\n         29906, 29900, 29896, 29941,   278,  3303,  3900,   322, 12710,  7450,\n           278, 16657,   363,  1260,   326,  3381,   310,  8713,  6392, 12677,\n           936,  1334,   481,   787,   607,   471,  1095,   943,   287,  2678,\n           373,   491,   278,  8291, 14223,  8831,   411, 10104, 29871, 29906,\n         29896, 29896, 29947, 29889, 29871, 29946, 29889,  6418, 29907, 29956,\n         29901,  1551, 29871, 29896, 29945,  3979, 29871, 29906, 29900, 29896,\n         29941,   278, 28841,  8831,   310,   278,  9205,  2133,   363,   278,\n          1019,  6335,   654,   310, 12677,   936,  1334,   481,   787,   313,\n          4590, 29907, 29956, 29897,  1602,  2247, 29901,   363,   599,   916,\n          8052, 22233, 29879, 29901,  6763,   310, 22104,   408,  4720,   408,\n          1950,   411, 13285,   310, 22104,   451,  2678,  1135, 29871, 29941,\n         29900,  5306, 29871, 29906, 29900, 29896, 29946, 29889,   450,  6418,\n         29907, 29956, 29899,  3904,   435,  2461, 18971,   471, 28269,  7841,\n           373, 29871, 29896, 29953,  5533, 29871, 29906, 29900, 29896, 29941,\n         29889, 29871, 29945, 29889,  4750,   287,  7123,  9012, 29901,  4001,\n           769,   278,  4007,   328, 22384,   756,  1063, 17644, 28886,   310,\n          8338,  3460,   967,  6900,  2534, 13726,   263,  3652,   310,  7123,\n          9012,  3704,   263,  7123,  1220,   304,  7751,   278,  1556,   454,\n           386,   284,   310, 22233,  7117,   714,   310,  8713,  2849,   491,\n           278,  1095,   310, 29871, 29906, 29900, 29896, 29941,   263,  6339,\n         29871, 29945, 29871, 29906, 29900, 29896, 29946,  7123,  1220,   304,\n          4337,   599, 22233, 29879, 28177,   363, 22104,   322,   263,  7123,\n          1220,   304,  8174,   599,   310,   967, 22233, 25340,  5802, 11840,\n           491, 29871, 29896, 29945,  4779, 29871, 29906, 29900, 29896, 29946,\n         29889, 29871, 29953, 29889, 29871, 29896, 29945, 27908, 10489, 16661,\n         29901,   450,  4007,   328, 22384,   756,  1304, 27908, 10489,   472,\n          3203, 29871, 29896, 29945,  3064,   297,  7786,  7378, 29889,   450,\n          6418, 29907, 29956,  9326,   373,   323,  1041,  3250,  3786, 29871,\n         29906, 29929, 29871, 29906, 29900, 29896, 29946,   393,   372,   674,\n          3638,   263,  2114, 29899,  2886,   292, 10655,   304, 23033,   278,\n          7786,   671,   310,   521,  5095,   457, 10489,   297,  8713,  2849,\n         29889, 29871, 29955, 29889,  3295,  1037,  8357,  2478, 29901,  1551,\n          5306, 29871, 29946, 29871, 29906, 29900, 29896, 29946,   830,   329,\n           414,  8967,   278,  2343,   310,   278,  6418, 29907, 29956, 29899,\n          3904,   435,  2461, 18971, 15861,  2429, 13680,   351, 17845,  2645,\n           263, 11473]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1])}"},"metadata":{}}]},{"cell_type":"code","source":"print(max(len(preprocessed_train_dataset[i]['input_ids']) for i in range(len(preprocessed_train_dataset))))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:14:08.892672Z","iopub.execute_input":"2024-03-26T22:14:08.893018Z","iopub.status.idle":"2024-03-26T22:14:09.091499Z","shell.execute_reply.started":"2024-03-26T22:14:08.892992Z","shell.execute_reply":"2024-03-26T22:14:09.090698Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"code","source":"################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 16\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 64\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n# Bias\nbias = \"none\"\n\n# Task type\n# task_type = \"CAUSAL_LM\" # basic\n# task_type = \"SEQ_CLS\" #class\ntask_type = TaskType.SEQ_CLS\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n\n# Batch size per GPU for training\nper_device_train_batch_size = 16\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 4\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-5\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\ntrain_epochs = 1\n\n# Linear warmup steps from 0 to learning_rate\nwarmup_steps = 2\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = True\n\n# Log every X updates steps\n# need to be multiples of gradient_accumulation_steps\nlogging_steps = 20\neval_steps = 20","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:14:11.333338Z","iopub.execute_input":"2024-03-26T22:14:11.333789Z","iopub.status.idle":"2024-03-26T22:14:11.341158Z","shell.execute_reply.started":"2024-03-26T22:14:11.333757Z","shell.execute_reply":"2024-03-26T22:14:11.340151Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def find_all_linear_names(model):\n    \"\"\"\n    Find modules to apply LoRA to.\n\n    :param model: PEFT model\n    \"\"\"\n\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            print(name)\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    print(f\"LoRA module names: {list(lora_module_names)}\")\n    return list(lora_module_names)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T16:22:52.235272Z","iopub.execute_input":"2024-03-26T16:22:52.235630Z","iopub.status.idle":"2024-03-26T16:22:52.242866Z","shell.execute_reply.started":"2024-03-26T16:22:52.235601Z","shell.execute_reply":"2024-03-26T16:22:52.241941Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def find_all_linear_names_new(model):\n    \"\"\"\n    Find modules to apply LoRA to.\n\n    :param model: PEFT model\n    \"\"\"\n\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            lora_module_names.add(name)\n\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    print(f\"LoRA module names: {list(lora_module_names)}\")\n    return list(lora_module_names)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T16:22:55.154822Z","iopub.execute_input":"2024-03-26T16:22:55.155239Z","iopub.status.idle":"2024-03-26T16:22:55.161378Z","shell.execute_reply.started":"2024-03-26T16:22:55.155209Z","shell.execute_reply":"2024-03-26T16:22:55.160342Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n    \"\"\"\n    Creates Parameter-Efficient Fine-Tuning configuration for the model\n\n    :param r: LoRA attention dimension\n    :param lora_alpha: Alpha parameter for LoRA scaling\n    :param modules: Names of the modules to apply LoRA to\n    :param lora_dropout: Dropout Probability for LoRA layers\n    :param bias: Specifies if the bias parameters should be trained\n    \"\"\"\n    config = LoraConfig(\n        r = r,\n        lora_alpha = lora_alpha,\n        target_modules = target_modules,\n        lora_dropout = lora_dropout,\n        bias = bias,\n        task_type = task_type,\n    )\n\n    return config","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:14:21.392153Z","iopub.execute_input":"2024-03-26T22:14:21.392866Z","iopub.status.idle":"2024-03-26T22:14:21.398170Z","shell.execute_reply.started":"2024-03-26T22:14:21.392829Z","shell.execute_reply":"2024-03-26T22:14:21.397066Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model, use_4bit = False):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n\n    :param model: PEFT model\n    \"\"\"\n\n    trainable_params = 0\n    all_param = 0\n\n    for _, param in model.named_parameters():\n        num_params = param.numel()\n        if num_params == 0 and hasattr(param, \"ds_numel\"):\n            num_params = param.ds_numel\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n\n    if use_4bit:\n        trainable_params /= 2\n\n    print(\n        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:14:23.757269Z","iopub.execute_input":"2024-03-26T22:14:23.758105Z","iopub.status.idle":"2024-03-26T22:14:23.764011Z","shell.execute_reply.started":"2024-03-26T22:14:23.758070Z","shell.execute_reply":"2024-03-26T22:14:23.763079Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(logits_and_labels):\n  logits, labels = logits_and_labels\n  predictions = np.argmax(logits, axis=-1)\n#   predictions = logits\n  acc = np.mean(predictions == labels)\n  precision = precision_score(labels, predictions, average='weighted')\n  recall = recall_score(labels, predictions, average='weighted')\n  f1 = f1_score(labels, predictions, average='weighted')\n  gmean = geometric_mean_score(labels, predictions, average='weighted')\n  return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1_score': f1, 'gmean': gmean}","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:14:26.752162Z","iopub.execute_input":"2024-03-26T22:14:26.752904Z","iopub.status.idle":"2024-03-26T22:14:26.763169Z","shell.execute_reply.started":"2024-03-26T22:14:26.752869Z","shell.execute_reply":"2024-03-26T22:14:26.762325Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# is not used\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        display(\"labels\", labels)\n        display(\"labels type\", labels.dtype)\n        display(self.model.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int))\n        \n        outputs = model(**inputs)\n        logits = outputs.logits\n#         logits = logits.cpu()\n        display(\"old logits\", logits)\n        logits = logits.cpu().detach().numpy()\n        logits_eager = tf.nn.softmax(logits, name=None)\n        new_logits = torch.from_numpy(logits_eager.numpy())\n        display(\"new logits\", new_logits)\n        \n#         display(\"logits\", logits)\n#         display(\"logits-view\", logits.view(-1, self.model.config.num_labels))\n#         logits = np.argmax(logits, axis=-1)\n#         value, index = torch.max(logits, 1)\n#         display(\"logits value\", value)\n#         value = torch.argmax(logits)\n#         display(\"logits value\", value)\n#         loss_fct = torch.nn.BCEWithLogitsLoss()\n        loss_fct = torch.nn.CrossEntropyLoss()\n#         loss = tf.nn.softmax_cross_entropy_with_logits(logits.cpu().detach().numpy(), labels.cpu().detach().numpy(), name=None)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels)\n        display(\"...loss\", loss)\n#         loss = loss_fct(logits, labels.unsqueeze(1))\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fine_tune(model,\n          tokenizer,\n          train_ds,\n          valid_ds,\n          lora_r,\n          lora_alpha,\n          lora_dropout,\n          bias,\n          task_type,\n          per_device_train_batch_size,\n          gradient_accumulation_steps,\n          warmup_steps,\n          max_steps,\n          learning_rate,\n          fp16,\n          logging_steps,\n          output_dir,\n          optim):\n    \"\"\"\n    Prepares and fine-tune the pre-trained model.\n\n    :param model: Pre-trained Hugging Face model\n    :param tokenizer: Model tokenizer\n    :param dataset: Preprocessed training dataset\n    \"\"\"\n\n    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n    model.gradient_checkpointing_enable()\n\n    # Prepare the model for training\n    model = prepare_model_for_kbit_training(model)\n\n    # Get LoRA module names\n#     target_modules = find_all_linear_names_new(model)\n    target_modules = []\n    for i in range(7):\n        target_modules += [f'model.layers.{i}.self_attn.q_proj',\n                      f'model.layers.{i}.self_attn.k_proj',\n                      f'model.layers.{i}.self_attn.v_proj',\n                      f'model.layers.{i}.self_attn.o_proj', \n                      f'model.layers.{i}.mlp.gate_proj',\n                      f'model.layers.{i}.mlp.up_proj',\n                      f'model.layers.{i}.mlp.down_proj']\n\n    print(target_modules)\n\n    # Create PEFT configuration for these modules and wrap the model to PEFT\n    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n    model = get_peft_model(model, peft_config)\n\n    # Print information about the percentage of trainable parameters\n    print_trainable_parameters(model)\n\n    # Training parameters\n    trainer = Trainer(\n        model = model,\n        train_dataset = train_ds,\n        eval_dataset = valid_ds,\n        compute_metrics=compute_metrics,\n        args = TrainingArguments(\n            num_train_epochs=train_epochs,\n            per_device_train_batch_size = per_device_train_batch_size,\n            gradient_accumulation_steps = gradient_accumulation_steps,\n            warmup_steps = warmup_steps,\n            # max_steps = max_steps,\n            learning_rate = learning_rate,\n            fp16 = fp16,\n            logging_steps = logging_steps,\n            output_dir = output_dir,\n            optim = optim,\n#             evaluation_strategy = \"steps\", # does evaluation during training\n            evaluation_strategy = \"epoch\", # does evaluation at the end of the epoch\n            # save_strategy = \"steps\",\n            eval_steps = eval_steps,\n            # save_steps = max_steps,\n            # report_to=\"tensorboard\"\n        ),\n        data_collator = DataCollatorWithPadding(tokenizer)\n    )\n\n    model.config.use_cache = False\n\n    model = torch.compile(model)\n\n    do_train = True\n\n    # Launch training and log metrics\n    print(\"Training...\")\n\n    if do_train:\n        train_result = trainer.train()\n        metrics = train_result.metrics\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n        print(metrics)\n\n        # compute evaluation results\n        # metrics = trainer.evaluate()\n        # # save evaluation results\n        # trainer.log_metrics(\"eval\", metrics)\n        # trainer.save_metrics(\"eval\", metrics)\n        # print(metrics)\n\n    # Save model\n    print(\"Saving last checkpoint of the model...\")\n    os.makedirs(output_dir, exist_ok = True)\n    trainer.model.save_pretrained(output_dir)\n\n    # Free memory for merging weights\n    del model\n    del trainer\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:17:01.619082Z","iopub.execute_input":"2024-03-26T22:17:01.619969Z","iopub.status.idle":"2024-03-26T22:17:01.632400Z","shell.execute_reply.started":"2024-03-26T22:17:01.619932Z","shell.execute_reply":"2024-03-26T22:17:01.631436Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"fine_tune(model, tokenizer, preprocessed_train_dataset, preprocessed_validate_dataset, lora_r, lora_alpha, lora_dropout, bias, task_type, per_device_train_batch_size, gradient_accumulation_steps, warmup_steps, max_steps, learning_rate, fp16, logging_steps, output_dir, optim)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:17:07.176008Z","iopub.execute_input":"2024-03-26T22:17:07.176722Z","iopub.status.idle":"2024-03-26T22:37:46.324603Z","shell.execute_reply.started":"2024-03-26T22:17:07.176692Z","shell.execute_reply":"2024-03-26T22:37:46.323440Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"['model.layers.0.self_attn.q_proj', 'model.layers.0.self_attn.k_proj', 'model.layers.0.self_attn.v_proj', 'model.layers.0.self_attn.o_proj', 'model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.down_proj', 'model.layers.1.self_attn.q_proj', 'model.layers.1.self_attn.k_proj', 'model.layers.1.self_attn.v_proj', 'model.layers.1.self_attn.o_proj', 'model.layers.1.mlp.gate_proj', 'model.layers.1.mlp.up_proj', 'model.layers.1.mlp.down_proj', 'model.layers.2.self_attn.q_proj', 'model.layers.2.self_attn.k_proj', 'model.layers.2.self_attn.v_proj', 'model.layers.2.self_attn.o_proj', 'model.layers.2.mlp.gate_proj', 'model.layers.2.mlp.up_proj', 'model.layers.2.mlp.down_proj', 'model.layers.3.self_attn.q_proj', 'model.layers.3.self_attn.k_proj', 'model.layers.3.self_attn.v_proj', 'model.layers.3.self_attn.o_proj', 'model.layers.3.mlp.gate_proj', 'model.layers.3.mlp.up_proj', 'model.layers.3.mlp.down_proj', 'model.layers.4.self_attn.q_proj', 'model.layers.4.self_attn.k_proj', 'model.layers.4.self_attn.v_proj', 'model.layers.4.self_attn.o_proj', 'model.layers.4.mlp.gate_proj', 'model.layers.4.mlp.up_proj', 'model.layers.4.mlp.down_proj', 'model.layers.5.self_attn.q_proj', 'model.layers.5.self_attn.k_proj', 'model.layers.5.self_attn.v_proj', 'model.layers.5.self_attn.o_proj', 'model.layers.5.mlp.gate_proj', 'model.layers.5.mlp.up_proj', 'model.layers.5.mlp.down_proj', 'model.layers.6.self_attn.q_proj', 'model.layers.6.self_attn.k_proj', 'model.layers.6.self_attn.v_proj', 'model.layers.6.self_attn.o_proj', 'model.layers.6.mlp.gate_proj', 'model.layers.6.mlp.up_proj', 'model.layers.6.mlp.down_proj']\nAll Parameters: 3,378,102,272 || Trainable Parameters: 8,761,344 || Trainable Parameters %: 0.2593569789943885\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240326_221745-ri2yytib</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/llama-fakenews/huggingface/runs/ri2yytib' target=\"_blank\">amber-bee-54</a></strong> to <a href='https://wandb.ai/llama-fakenews/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/llama-fakenews/huggingface' target=\"_blank\">https://wandb.ai/llama-fakenews/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/llama-fakenews/huggingface/runs/ri2yytib' target=\"_blank\">https://wandb.ai/llama-fakenews/huggingface/runs/ri2yytib</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 17:36, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n      <th>Gmean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>1.514375</td>\n      <td>0.475000</td>\n      <td>0.472059</td>\n      <td>0.475000</td>\n      <td>0.472348</td>\n      <td>0.471162</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"***** train metrics *****\n  epoch                    =       0.98\n  total_flos               = 11874450GF\n  train_loss               =     1.5946\n  train_runtime            = 0:20:22.69\n  train_samples_per_second =      0.526\n  train_steps_per_second   =      0.008\n{'train_runtime': 1222.6959, 'train_samples_per_second': 0.526, 'train_steps_per_second': 0.008, 'total_flos': 1.27500936019968e+16, 'train_loss': 1.5945724487304687, 'epoch': 0.98}\nSaving last checkpoint of the model...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SAVING TO HF\n*restart, import all dependencies*\n","metadata":{}},{"cell_type":"code","source":"label2id = {'TRUE': 0, 'FAKE': 1}\nid2label = {0: 'TRUE', 1: 'FAKE'}\n\n\n# Load fine-tuned weights\nmodel = AutoPeftModelForSequenceClassification.from_pretrained(\n    \"FakeNewsLlama/CrimeTrueFake7Layer\", device_map = \"auto\", torch_dtype = torch.bfloat16, offload_folder='./', num_labels=2, label2id=label2id, id2label=id2label)\n\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel.config.problem_type = \"single_label_classification\"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T14:55:45.733651Z","iopub.execute_input":"2024-03-26T14:55:45.734559Z","iopub.status.idle":"2024-03-26T14:57:55.659116Z","shell.execute_reply.started":"2024-03-26T14:55:45.734521Z","shell.execute_reply":"2024-03-26T14:57:55.658044Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5430ea0c8dfb43cd8429431f692ecde1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ca98cf6bbd045678c421ee6b0deb667"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ade7206d5e41fa9951b0da38437749"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88c6e5ecd17844b784c5f031cdf67392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e4183079a00467daf484b9ef97ef331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db7a85ad65c48e9b06d2a32979d17b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7187e0774fdd4620b6c6b263ef89b294"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/3.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe1c5294ded4b8e9ecccc5870c0bc09"}},"metadata":{}}]},{"cell_type":"code","source":"def evaluate_model(model,\n          tokenizer,\n          train_ds,\n          valid_ds,\n          lora_r,\n          lora_alpha,\n          lora_dropout,\n          bias,\n          task_type,\n          per_device_train_batch_size,\n          gradient_accumulation_steps,\n          warmup_steps,\n          max_steps,\n          learning_rate,\n          fp16,\n          logging_steps,\n          output_dir,\n          optim):\n    \"\"\"\n    Prepares and fine-tune the pre-trained model.\n\n    :param model: Pre-trained Hugging Face model\n    :param tokenizer: Model tokenizer\n    :param dataset: Preprocessed training dataset\n    \"\"\"\n    # Training parameters\n    trainer = Trainer(\n        model = model,\n        train_dataset = train_ds,\n#         eval_dataset = valid_ds.shuffle(seed=42).select(range(400)),\n        eval_dataset = valid_ds,\n        compute_metrics=compute_metrics,\n        args = TrainingArguments(\n            num_train_epochs=train_epochs,\n            per_device_train_batch_size = per_device_train_batch_size,\n            gradient_accumulation_steps = gradient_accumulation_steps,\n            warmup_steps = warmup_steps,\n            # max_steps = max_steps,\n            learning_rate = learning_rate,\n            fp16 = fp16,\n            logging_steps = logging_steps,\n            output_dir = output_dir,\n            optim = optim,\n            evaluation_strategy = \"steps\", # does evaluation during training\n            # evaluation_strategy = \"epoch\", # does evaluation at the end of the epoch\n            # save_strategy = \"steps\",\n            eval_steps = eval_steps,\n            # save_steps = max_steps,\n            # report_to=\"tensorboard\"\n        ),\n        data_collator = DataCollatorWithPadding(tokenizer)\n    )\n\n    model.config.use_cache = False\n\n    model = torch.compile(model)\n    torch.backends.cuda.enable_mem_efficient_sdp(False)\n    torch.backends.cuda.enable_flash_sdp(False)\n\n    do_train = True\n\n    # Launch training and log metrics\n    print(\"Training...\")\n\n    if do_train:\n        #         train_result = trainer.train()\n        #         metrics = train_result.metrics\n        #         trainer.log_metrics(\"train\", metrics)\n        #         trainer.save_metrics(\"train\", metrics)\n        #         trainer.save_state()\n        #         print(metrics)\n        # compute evaluation results\n        metrics = trainer.evaluate()\n        # save evaluation results\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        print(metrics)\n\n# Save model\n# print(\"Saving last checkpoint of the model...\")\n# os.makedirs(output_dir, exist_ok = True)\n# trainer.model.save_pretrained(output_dir)\n\n    # Free memory for merging weights\n    del model\n    del trainer\n    torch.cuda.empty_cache()\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:01:08.866872Z","iopub.execute_input":"2024-03-26T15:01:08.867300Z","iopub.status.idle":"2024-03-26T15:01:08.882890Z","shell.execute_reply.started":"2024-03-26T15:01:08.867264Z","shell.execute_reply":"2024-03-26T15:01:08.879822Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model, tokenizer, preprocessed_train_dataset, preprocessed_validate_dataset, lora_r, lora_alpha, lora_dropout, bias, task_type, per_device_train_batch_size, gradient_accumulation_steps, warmup_steps, max_steps, learning_rate, fp16, logging_steps, output_dir, optim)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:01:12.687033Z","iopub.execute_input":"2024-03-26T15:01:12.687896Z","iopub.status.idle":"2024-03-26T15:02:49.179704Z","shell.execute_reply.started":"2024-03-26T15:01:12.687864Z","shell.execute_reply":"2024-03-26T15:02:49.178288Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:31]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240326_150218-zyz66pi4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/llama-fakenews/huggingface/runs/zyz66pi4' target=\"_blank\">vibrant-mountain-49</a></strong> to <a href='https://wandb.ai/llama-fakenews/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/llama-fakenews/huggingface' target=\"_blank\">https://wandb.ai/llama-fakenews/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/llama-fakenews/huggingface/runs/zyz66pi4' target=\"_blank\">https://wandb.ai/llama-fakenews/huggingface/runs/zyz66pi4</a>"},"metadata":{}},{"name":"stdout","text":"***** eval metrics *****\n  eval_accuracy           =      0.375\n  eval_f1_score           =     0.3719\n  eval_gmean              =     0.3784\n  eval_loss               =     1.5601\n  eval_precision          =     0.3763\n  eval_recall             =      0.375\n  eval_runtime            = 0:00:36.66\n  eval_samples_per_second =      2.182\n  eval_steps_per_second   =      0.273\n{'eval_loss': 1.5600738525390625, 'eval_accuracy': 0.375, 'eval_precision': 0.37627877237851665, 'eval_recall': 0.375, 'eval_f1_score': 0.3718671679197995, 'eval_gmean': 0.37843042486851586, 'eval_runtime': 36.6684, 'eval_samples_per_second': 2.182, 'eval_steps_per_second': 0.273}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_validate_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[34], line 89\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\n\u001b[1;32m     87\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'model' referenced before assignment","output_type":"error"}]},{"cell_type":"code","source":"# Merge the LoRA layers with the base model\nmodel = model.merge_and_unload()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only to remove directories\nimport shutil\n# shutil.rmtree('/kaggle/working/news_classification_llama2_7b/final_merged_checkpoint')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save fine-tuned model at a new location\noutput_merged_dir = \"/kaggle/working/news_classification_llama2_7b/final_merged_checkpoint2\"\nos.makedirs(output_merged_dir, exist_ok = True)\nmodel.save_pretrained(output_merged_dir, safe_serialization = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_model = \"FakeNewsLlama/CrimeTrueFakeClassifier_C1\"\nmodel.push_to_hub(merged_model, use_auth_token = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save tokenizer for easy inference\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.save_pretrained(output_merged_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.push_to_hub(merged_model, use_auth_token = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING\n*restart only*","metadata":{}},{"cell_type":"code","source":"!pip install accelerate==0.27.2 --progress-bar off\n!pip install -q peft==0.4.0 --progress-bar off\n# !pip install -q bitsandbytes==0.40.2 --progress-bar off\n!pip install -q bitsandbytes==0.41.3 --progress-bar off\n!pip install -q transformers==4.38.2 --progress-bar off\n!pip install -q trl==0.4.7 --progress-bar off\n\n!pip install datasets==2.18.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom random import randrange\nfrom functools import partial\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (AutoModelForCausalLM,\n                          AutoModelForSequenceClassification,\n                          AutoTokenizer,\n                          BitsAndBytesConfig,\n                          HfArgumentParser,\n                          Trainer,\n                          TrainingArguments,\n                          DataCollatorForLanguageModeling,\n                          DataCollatorWithPadding,\n                          EarlyStoppingCallback,\n                          pipeline,\n                          logging,\n                          set_seed)\n\nimport bitsandbytes as bnb\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification, TaskType\nfrom trl import SFTTrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nfrom datasets import load_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Get number of GPU device and set maximum memory\nn_gpus = torch.cuda.device_count()\nmax_memory = f'{40960}MB'\ntrained_model_name = \"FakeNewsLlama/CrimeTrueFakeClassifier_C1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_file_path = \"/kaggle/input/crime-fakenews-data/test_crime_df.csv\"\ntest_ds = load_dataset(\"csv\", data_files=test_file_path)[\"train\"]\n\ntest_ds_subset = test_ds[:2]\ntest_ds_subset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompting using validation set\nresults = pd.DataFrame(columns = ['id', 'text', 'expected_domain', 'predicted_domain'])\n\ngen = pipeline('text-classification', model=trained_model_name)\npredicted_ds = gen(test_ds_subset[\"text\"])\n# output in form: [{'label': 'TRUE', 'score': 0.6169524788856506},...]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id = 0\nresults = pd.DataFrame(columns = ['id', 'text', 'expected_domain', 'predicted_domain'])\n\nfor predicted in predicted_ds:\n    results.loc[id] = [str(id), test_ds_subset['text'][id], test_ds_subset['label'][id], predicted['label']]\n    id += 1\n\ndisplay(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/tests/Crime_C1/\", exist_ok = True)\nresults.to_csv(\"/kaggle/working/tests/Crime_C1/crime_test_results.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}